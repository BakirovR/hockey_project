{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a903affc",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Creating-dataframe-№1\" data-toc-modified-id=\"Creating-dataframe-№1-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Creating dataframe №1</a></span></li><li><span><a href=\"#Creating-dataframe-№2\" data-toc-modified-id=\"Creating-dataframe-№2-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Creating dataframe №2</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f97977e",
   "metadata": {},
   "source": [
    "Our goal is to create two dataframes for further analysis in pandas by parsing the website https://en.khl.ru.\n",
    "\n",
    "First, we create dataframe №1 with time intervals during which the player represented a particular club:\n",
    "\n",
    "- We iterate through 125 pages to gather the ID of each player;\n",
    "\n",
    "- We generate links to each player's page and determine the number of pages containing data on game dates for specific clubs;\n",
    "\n",
    "- We navigate through each page of each player and collect the performance dates for clubs;\n",
    " \n",
    "- We write the data to a CSV file.\n",
    "\n",
    "Dataframe №2 reflects the statistics of each player throughout their career:\n",
    "\n",
    "- We navigate through the pages for each KHL player and collect statistical data;\n",
    "\n",
    "- We write the data to a CSV file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462accf7",
   "metadata": {},
   "source": [
    "## Creating dataframe №1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ada76758",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T06:39:12.998617Z",
     "start_time": "2023-10-15T06:39:12.762517Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from bs4 import BeautifulSoup  \n",
    "import requests  \n",
    "from tqdm import tqdm  \n",
    "import csv\n",
    "from concurrent.futures import ThreadPoolExecutor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "651e98ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T06:39:48.695532Z",
     "start_time": "2023-10-15T06:39:13.726800Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 125/125 [00:34<00:00,  3.59it/s]\n",
      "100%|███████████████████████████████████| 3750/3750 [00:00<00:00, 681956.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generating links to player pages\n",
    "links = [f'https://en.khl.ru/players/season/all/?season=all&pager_selector=&PAGEN_1={x}' for x in range(1, 126)]\n",
    "\n",
    "# Function to process a single link and generate player links\n",
    "def process_link(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        response.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        player_entries = soup.find('tbody', id='player_list_container').find_all('a', class_='players-table__player')\n",
    "        player_links = [f\"https://en.khl.ru{entry['href']}\" for entry in player_entries]\n",
    "        return player_links\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Using ThreadPoolExecutor to parallelize link processing\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    results = list(tqdm(executor.map(process_link, links), total=len(links)))\n",
    "\n",
    "# Flatten the list of player links\n",
    "player_links = [link for sublist in results for link in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3af4b608",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T13:33:10.271823Z",
     "start_time": "2023-10-15T13:33:10.247741Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 3750/3750 [00:00<00:00, 647215.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate player links with IDs and page number\n",
    "player_links_new = [f\"{el}?idplayer={el.split('/')[-2]}&PAGEN_1=1\" for el in tqdm(player_links)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd68720f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T14:03:13.435683Z",
     "start_time": "2023-10-15T13:33:19.927720Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3750/3750 [29:53<00:00,  2.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# Forming a dictionary where each player's link corresponds to a list of links to all their KHL game pages.\n",
    "# The function process_url takes a URL as input and performs the following actions:\n",
    "# Sends an HTTP GET request to the specified URL.\n",
    "# Decodes the response content using UTF-8 encoding.\n",
    "# Creates a BeautifulSoup object for parsing the HTML content of the response.\n",
    "# Searches for a specific div element with the class 'pagging judges-tableStat__pagging' in the HTML.\n",
    "# Extracts the last element of the text inside the pagen element (which is assumed to represent the number of pages on the website). If the element is not found (i.e., pagen is None), the function returns 1 (assuming there is only one page on the website).\n",
    "# ThreadPoolExecutor is used to create a pool of worker threads with a maximum number of threads set to 10 (specified as max_workers=10).\n",
    "# Calling the function executor.submit(process_url, url) sends the function process_url with each URL from the player_links_new list as an argument and returns a future object representing the computation result.\n",
    "# The list futures contains all the future objects representing pending or completed computations.\n",
    "# The for loop iterates over both future and URL simultaneously using tqdm to display a progress indicator with a progress bar.\n",
    "# For each future, future.result() is called to retrieve the result of the corresponding computation (the number of pages obtained from process_url).\n",
    "# Then the dictionary mydict is updated, where the keys are URLs and the values are the results (number of pages).\n",
    "def process_url(url):\n",
    "    response = requests.get(url=url)\n",
    "    response.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    pagen = soup.find('div', {'class': 'pagging judges-tableStat__pagging'})\n",
    "    return int(pagen.text.strip().split('\\n')[-1]) if pagen else 1\n",
    "\n",
    "mydict = {}\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = [executor.submit(process_url, url) for url in player_links_new]\n",
    "    for future, url in tqdm(zip(futures, player_links_new), total=len(player_links_new)):\n",
    "        result = future.result()\n",
    "        mydict[url] = result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b02f7d9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T14:03:13.456270Z",
     "start_time": "2023-10-15T14:03:13.441102Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store the result\n",
    "result_dict = {}\n",
    "\n",
    "# Loop through the items in mydict\n",
    "for k, v in mydict.items():\n",
    "    # Check if the value v is greater than 1\n",
    "    if v > 1:\n",
    "        # If v is greater than 1, create a list of URLs with a range of numbers\n",
    "        urls = [k[:-1] + str(i) for i in range(1, v+1)]\n",
    "        # Store the list of URLs in the result_dict under the key k\n",
    "        result_dict[k] = urls\n",
    "    else:\n",
    "        # If v is not greater than 1, create a list with a single URL (the original key k)\n",
    "        # Store the list with a single URL in the result_dict under the key k\n",
    "        result_dict[k] = [k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99942706",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T14:03:13.461536Z",
     "start_time": "2023-10-15T14:03:13.458230Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.khl.ru/players/13705/?idplayer=13705&PAGEN_1=1',\n",
       " 'https://en.khl.ru/players/13705/?idplayer=13705&PAGEN_1=2',\n",
       " 'https://en.khl.ru/players/13705/?idplayer=13705&PAGEN_1=3',\n",
       " 'https://en.khl.ru/players/13705/?idplayer=13705&PAGEN_1=4',\n",
       " 'https://en.khl.ru/players/13705/?idplayer=13705&PAGEN_1=5',\n",
       " 'https://en.khl.ru/players/13705/?idplayer=13705&PAGEN_1=6',\n",
       " 'https://en.khl.ru/players/13705/?idplayer=13705&PAGEN_1=7',\n",
       " 'https://en.khl.ru/players/13705/?idplayer=13705&PAGEN_1=8',\n",
       " 'https://en.khl.ru/players/13705/?idplayer=13705&PAGEN_1=9',\n",
       " 'https://en.khl.ru/players/13705/?idplayer=13705&PAGEN_1=10',\n",
       " 'https://en.khl.ru/players/13705/?idplayer=13705&PAGEN_1=11',\n",
       " 'https://en.khl.ru/players/13705/?idplayer=13705&PAGEN_1=12',\n",
       " 'https://en.khl.ru/players/13705/?idplayer=13705&PAGEN_1=13',\n",
       " 'https://en.khl.ru/players/13705/?idplayer=13705&PAGEN_1=14',\n",
       " 'https://en.khl.ru/players/13705/?idplayer=13705&PAGEN_1=15']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the result\n",
    "result_dict['https://en.khl.ru/players/13705/?idplayer=13705&PAGEN_1=1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd53c20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T16:31:41.042959Z",
     "start_time": "2023-10-14T16:31:41.035418Z"
    }
   },
   "outputs": [],
   "source": [
    "def player_team(links):\n",
    "    commands = []  # Initialize a list to store player teams and game dates\n",
    "\n",
    "    for url in links:\n",
    "        response = requests.get(url=url)\n",
    "        response.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "        # Extract the player's name from the web page\n",
    "        player = soup.find('span', {'class': 'frameCard-header__detail-titleItem roboto'}).text\n",
    "\n",
    "        pagen = soup.find('tbody', id='table_all_games')\n",
    "        dates = pagen.find_all('td', {'class': 'matches-table__col matches-table__col_date'})\n",
    "        teams = pagen.find_all('td', {'class': 'matches-table__col matches-table__col_name'})\n",
    "\n",
    "        # Extract teams and corresponding dates for the player's games\n",
    "        for x, y in zip(teams, dates):\n",
    "            commands.append((x.find('strong').text, y.text.strip('\\n')))\n",
    "\n",
    "    dict_list = []  # Initialize a list to store dictionaries with team-game date pairs\n",
    "    current_dict = {}\n",
    "\n",
    "    for command, date in commands:\n",
    "        current_dict.setdefault(command, []).append(date)\n",
    "\n",
    "        if len(current_dict) > 1:\n",
    "            key, values = current_dict.popitem()\n",
    "            new_dict = {k: [v[-1], v[0]] for k, v in current_dict.items()}\n",
    "            dict_list.append(new_dict)\n",
    "            current_dict.clear()\n",
    "            current_dict[key] = values\n",
    "\n",
    "    # Add the last dictionary to the list\n",
    "    if current_dict:\n",
    "        new_dict = {k: [v[-1], v[0]] for k, v in current_dict.items()}\n",
    "        dict_list.append(new_dict)\n",
    "\n",
    "    # Create a final list of tuples with player info, team, and game dates\n",
    "    final_list = [(links[0], player, k, v[0], v[1]) for el in dict_list for k, v in el.items()]\n",
    "\n",
    "    return final_list  # Return the list of player-team-game date tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4e61ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T06:21:13.546654Z",
     "start_time": "2023-10-15T06:20:30.356001Z"
    }
   },
   "outputs": [],
   "source": [
    "lst_error = []  # Initialize a list to store links that result in errors during processing\n",
    "\n",
    "def process_links(links):\n",
    "    try:\n",
    "        return player_team(links)  # Call the player_team function to process the links and return the result\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing links: {e}\")\n",
    "        lst_error.append(links)  # Log the links that resulted in errors\n",
    "        return []  # Return an empty list in case of an error\n",
    "\n",
    "def process_result_dict(result_dict):\n",
    "    lst_final = [('player_link', 'player', \"team\", \"start_date\", 'end_date')]  # Initialize the final list with column headers\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(process_links, links) for k, links in result_dict.items()]  # Create futures for link processing\n",
    "\n",
    "        for future in tqdm(futures, total=len(result_dict)):\n",
    "            lst_final += future.result()  # Append the processed data to the final list\n",
    "\n",
    "process_result_dict(result_dict)  # Process the result_dict\n",
    "\n",
    "with open('khl_timedelta.csv', 'a', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(lst_final)  # Write the data to a CSV file\n",
    "\n",
    "print(\"Writing complete\")  # Print a completion message after writing to the CSV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d10ce1",
   "metadata": {},
   "source": [
    "## Creating dataframe №2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273cb9f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T06:26:15.677527Z",
     "start_time": "2023-10-15T06:25:59.501431Z"
    }
   },
   "outputs": [],
   "source": [
    "lst_info = [('player_link', 'player', 'position', 'born', 'age', 'country', 'hight', 'weight', 'shoot', 'GP', 'G', 'Assists', 'PTS', '+/-', '+', '-', 'PIM', 'ESG', 'PPG', 'SHG', 'OTG', 'GWG', 'SDS', 'SOG', '%SOG', 'S/G', 'FO', 'FOW', '%FO', 'TOI/G', 'SFT/G', 'TIE/G', 'SFTE/G', 'TIPP/G', 'SFTPP/G', 'TISH/G', 'SFTSH/G', 'HITS', 'BLS', 'FOA', 'TkA')]\n",
    "\n",
    "# Initialize a session for making HTTP requests\n",
    "session = requests.Session()\n",
    "\n",
    "# Set user-agent headers to mimic a web browser\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "session.headers.update(headers)\n",
    "\n",
    "# Function to process a player's information page\n",
    "def process_url(url):\n",
    "    try:\n",
    "        response = session.get(url=url)\n",
    "        response.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "        # Extract player details such as name, position, and personal information\n",
    "        player = extract_text(soup.find('span', {'class': 'frameCard-header__detail-titleItem roboto'}))\n",
    "        position = extract_text(soup.find('p', {'class': 'frameCard-header__detail-local roboto roboto-normal roboto-xxl color-black'}))\n",
    "        player_info = soup.find('div', {'class': 'frameCard-header__detail-body'}).find_all('p', {'class': 'roboto roboto-bold roboto-lg roboto-lg__lg-bigger color-black'})\n",
    "        player_info = [extract_text(info) for info in player_info]\n",
    "        born, age, country, hight, weight, shoots = player_info[:6]\n",
    "\n",
    "        # Extract statistical information for the player\n",
    "        stat = [extract_text(el) for el in soup.find('div', class_='detail-table').find_all('tr')[-1] if el.text.strip('\\n') != ''][1:]\n",
    "\n",
    "        return (url, player, position, born, age, country, hight, weight, shoots, *stat)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while processing {url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Function to extract text from an HTML element or return an empty string if the element is None\n",
    "def extract_text(element):\n",
    "    return element.text.strip('\\n') if element is not None else ''\n",
    "\n",
    "# Process player information pages using ThreadPoolExecutor\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results = list(tqdm(executor.map(process_url, player_links_new), total=len(player_links_new)))\n",
    "\n",
    "# Remove None values from results\n",
    "results = [result for result in results if result is not None]\n",
    "\n",
    "# Append player information to the CSV file\n",
    "with open('khl_info.csv', 'a', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(lst_info)\n",
    "\n",
    "print(\"Writing complete\")  # Print a completion message after writing to the CSV\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
